# -*- coding: utf-8 -*-
"""model_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KOTcorxhtKesz0pK2jv9BHOZwM5rGStB

# Model example

This example will show you the whole process from model testing, analysis, and SHAP.

Because data_preprocessing need raw data, so "example" folder has given you the output data and model.

> x_train.npy, x_test.npy, y_test.npy -> output of data_preprocessing

> model.pt -> model after training (model class and setting included in code)

If you didn't download the folder, must execute these two cells.

Start from here
"""



# Commented out IPython magic to ensure Python compatibility.
import shap
import pandas as pd
import numpy as np
import torch, torchvision
from torchvision import datasets, transforms
from torch import nn, optim
from torch.nn import functional as F
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import os
import argparse

"""超參數"""
parser = argparse.ArgumentParser(prog='model_example.py', description='SHAP example')
parser.add_argument('--partNo', '-N', default='0162B00100', type=str,  help='Part number for machine')
parser.add_argument('--model_name', '-m', default='model.pt', type=str,  help='which model in data folder do you want')
parser.add_argument('--x_train', '-xtr', default='x_train.npy', type=str,  help='which x_train in data folder do you want')
parser.add_argument('--x_test', '-xte', default='x_test.npy', type=str,  help='which x_test in data folder do you want')
parser.add_argument('--y_test', '-yte', default='y_test.npy', type=str,  help='which y_test in data folder do you want')

args = parser.parse_args()

config={
    'partNo':args.partNo,
    'folder_path':'./data/',
    'model_name':args.model_name,
    'x_train':args.x_train,
    'x_test':args.x_train,
    'y_test':args.x_train,
    'save_path':'./SHAP/',
    'features':['Speed', 'Frequency', 'Status'],
    'image_path':'image/',
    'detail_num':13

}
config['save_path']=config['save_path']+config['partNo']+'/'
config['image_path']=config['save_path']+config['image_path']

if not os.path.isdir(config['save_path']):
    os.makedirs(config['save_path'])
if not os.path.isdir(config['image_path']):
    os.makedirs(config['image_path'])

partNo = config['partNo'] #料號編號
# put your folder path, ex: '/content/drive/MyDrive/.../'

# put model name, ex: 'model.pt' for model path
model_path = config['folder_path']+config['model_name']

# put testing data and label npy file path, ex: 'x_test.npy', 'y_test.npy'
X_train = np.load(config['folder_path'] + config['x_train'])
X_test = np.load(config['folder_path'] + config['x_test'])
Y_test = np.load(config['folder_path'] + config['y_test'])
sequence_num=X_test.shape[1]
print("Sequence number: ", sequence_num)
print(f'X_test.shape : {X_test.shape}') #(testing_data_num, sequence_num, feature_num)
print(f'Y_test.shape : {Y_test.shape}') #(testing_data_num, output_num)
print('-'*20)
"""#Load model
Model class and setting for load model.
If you testing after training model, you can pass.
"""

"""
model class, should cpoy from training model
"""
class BiLSTM_layer(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, bidirectional, batch_first=False):
      super(BiLSTM_layer, self).__init__()
      self.lstm = nn.LSTM(
          input_size=input_size,
          hidden_size=hidden_size,
          num_layers=num_layers,
          bidirectional=bidirectional,
          batch_first=batch_first
      )

      self.fc = nn.Linear(hidden_size, 26)
      

  def forward(self, inputs):
      out, (h_n, c_n) = self.lstm(inputs, None)
      outputs = self.fc(torch.mean(h_n.squeeze(0), dim=0))

      return outputs

class DataEncoder(nn.Module):
  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):
    super(DataEncoder, self).__init__()
    self.input_dim = input_dim
    self.output_dim = output_dim
    
    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),
                              nn.ReLU(),
                              nn.Dropout(dropout),
                              nn.Linear(hidden_dim, output_dim)
                            )
  def forward(self, x):
    return self.net(x)

class minmax_RuleEncoder(nn.Module):
  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):
    super(minmax_RuleEncoder, self).__init__()
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),
                             nn.ReLU(),
                             nn.Dropout(dropout),
                             nn.Linear(hidden_dim, output_dim)
                            )

  def forward(self, x):
    return self.net(x)
    

class outbound_RuleEncoder(nn.Module):
  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):
    super(outbound_RuleEncoder, self).__init__()
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),
                             nn.ReLU(),
                             nn.Dropout(dropout),
                             nn.Linear(hidden_dim, output_dim)
                            )

  def forward(self, x):
    return self.net(x)
    
class DataonlyNet(nn.Module):
  def __init__(self, input_dim, output_dim, data_encoder, hidden_dim=4, n_layers=2, skip=False, input_type='state'):
    super(DataonlyNet, self).__init__()
    self.skip = skip
    self.input_type = input_type
    self.data_encoder = data_encoder
    self.n_layers = n_layers
    self.input_dim_decision_block = self.data_encoder.output_dim

    self.net = []
    for i in range(n_layers):
      if i == 0:
        in_dim = self.input_dim_decision_block
      else:
        in_dim = hidden_dim

      if i == n_layers-1:
        out_dim = output_dim
      else:
        out_dim = hidden_dim

      #self.net.append(nn.Linear(in_dim, out_dim))
      
      self.net.append(BiLSTM_layer(
              input_size=in_dim,
              hidden_size=64,
              num_layers=1,
              bidirectional=True,
              batch_first=True
          ))
      
      # self.net.append(nn.Flatten())
      # self.net.append(nn.Linear(64, 26))
      '''
      if i != n_layers-1:
        self.net.append(nn.ReLU())
      '''
      
    #self.net.append(nn.ReLU())
    self.net = nn.Sequential(*self.net)

  def get_z(self, x, alpha=0.0):
    data_z = self.data_encoder(x)

    return data_z

  def forward(self, x, alpha=0.0):    
      data_z = self.data_encoder(x)
      z=data_z
      if self.skip:
        if self.input_type == 'seq':
          return self.net(z) + x[:,-1,:]
        else:
          return self.net(z) + x    
      else:
        return self.net(z) 

class Net(nn.Module):
  def __init__(self, input_dim, output_dim, minmax_rule_encoder, outbound_rule_encoder, data_encoder, hidden_dim=3, n_layers=1, merge='cat', skip=False, input_type='state'):
    super(Net, self).__init__()
    self.skip = skip
    self.input_type = input_type
    self.minmax_rule_encoder = minmax_rule_encoder
    self.outbound_rule_encoder = outbound_rule_encoder
    self.data_encoder = data_encoder
    self.n_layers = n_layers
    assert self.minmax_rule_encoder.input_dim ==  self.data_encoder.input_dim
    assert self.minmax_rule_encoder.output_dim ==  self.data_encoder.output_dim
    self.merge = merge
    if merge == 'cat':
      self.input_dim_decision_block = self.minmax_rule_encoder.output_dim * 3
    elif merge == 'add':
      self.input_dim_decision_block = self.minmax_rule_encoder.output_dim

    self.net = []
    for i in range(n_layers):
      if i == 0:
        in_dim = self.input_dim_decision_block
      else:
        in_dim = hidden_dim

      if i == n_layers-1:
        out_dim = output_dim
      else:
        out_dim = hidden_dim

      #self.net.append(nn.Linear(in_dim, out_dim))
      
      self.net.append(BiLSTM_layer(
              input_size=in_dim,
              hidden_size=64,
              num_layers=1,
              bidirectional=True,
              batch_first=True
          ))
      
      # self.net.append(nn.Flatten())
      # self.net.append(nn.Linear(64, 26))
     
      
    #self.net.append(nn.ReLU())
    self.net = nn.Sequential(*self.net)

  def get_z(self, x, alpha=0.1, beta=0.1):
    minmax_rule_z = self.minmax_rule_encoder(x)
    outbound_rule_z = self.outbound_rule_encoder(x)
    data_z = self.data_encoder(x)

    if self.merge=='add':
      z = alpha*minmax_rule_z + beta*outbound_rule_z + (1-alpha-beta)*data_z    # merge: Add
    elif self.merge=='cat':
      z = torch.cat((alpha*minmax_rule_z , beta*outbound_rule_z , (1-alpha-beta)*data_z), dim=-1)    # merge: Concat
    elif self.merge=='equal_cat':
      z = torch.cat((minmax_rule_z,outbound_rule_z, data_z), dim=-1)    # merge: Concat

    return z

  def forward(self, x, alpha=0.1, beta=0.1):
      # merge: cat or add
      minmax_rule_z = self.minmax_rule_encoder(x)
      outbound_rule_z = self.outbound_rule_encoder(x)
      data_z = self.data_encoder(x)

      if self.merge=='add':
        z = alpha*minmax_rule_z + beta*outbound_rule_z + (1-alpha-beta)*data_z    # merge: Add
      elif self.merge=='cat':
        z = torch.cat((alpha*minmax_rule_z , beta*outbound_rule_z , (1-alpha-beta)*data_z), dim=-1)    # merge: Concat
      elif self.merge=='equal_cat':
        z = torch.cat((minmax_rule_z, outbound_rule_z, data_z), dim=-1)    # merge: Concat
      else:
        print(self.merge)
        print('-'*20)
      
      # z = torch.flatten(z, start_dim=1)

      if self.skip:
        if self.input_type == 'seq':
          return self.net(z) + x[:,-1,:]
        else:
          return self.net(z) + x    
      else:
        return self.net(z) 
        
        try:
          return self.net(z)   
        except:
          print(type(z))
          print(z.shape)

"""
model setting, should cpoy from training model
"""
merge = 'cat'
input_dim = 3
input_dim_encoder = 3
output_dim_encoder = 2
hidden_dim_encoder = 64
hidden_dim_db = 64
output_dim_encoder = output_dim_encoder
hidden_dim_encoder = hidden_dim_encoder
hidden_dim_db = hidden_dim_db
output_dim = 26
n_layers = 1
use_type=''


outbound_rule_encoder = outbound_RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.3)
minmax_rule_encoder = minmax_RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.3)
data_encoder = DataEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.3)

if use_type=='no_rule':
  model = Net(input_dim, output_dim, minmax_rule_encoder, outbound_rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge)
else:
  model =DataonlyNet (input_dim, output_dim, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers)

"""
Load model, and change X_test to tensor
"""
model = torch.load(model_path)
model.eval()
X_test = torch.tensor(X_test).to(torch.float32).cuda()

"""#SHAP """

def shap_calculator(train, test):
  X_train = torch.tensor(train).to(torch.float32).cuda()
  X_test = torch.tensor(test).to(torch.float32).cuda()
  print(X_test.shape)
  print('-'*20)
  features = config['features']

  #should only something like 100 or 1000 random background samples, not the whole training dataset.
  explainer = shap.DeepExplainer(model.cuda(), X_train[:sequence_num]) 
  shap_values_all = explainer.shap_values(X_test)
  np.save(config['save_path']+'shap.npy', shap_values_all)

  X = np.array(shap_values_all).reshape(-1,sequence_num,len(features)) #reshape the data for plot
  print(X.shape)
  print('-'*20)
  shap.summary_plot(X[:, 0, :], X_test[:][:, 0, :], plot_type="bar", feature_names = features, plot_size=(15,5))

"""
SHAP calculator using 'x_test.npy' for example
"""
#X_test = np.load('data/x_test.npy')
#X_train= np.load('data/x_train.npy')
#model = torch.load('data/model_DeepCTRLtesting2.pt')
torch.backends.cudnn.enabled=False
shap_calculator(X_train, X_test)

"""#SHAP heatmap picture from load data"""

"""
Calculator each feature SHAP value, and the sum of SHAP value
"""
import seaborn as sns

def shap_feature_calculator(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))

  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=1)
  shap_values = np.sum(shap_values, axis=0)
  shap_values = np.sum(shap_values, axis=0)

  num = 0
  
  for num in range(len(shap_values)):
    value = shap_values[num]
    values.append(value)

  shap_values_sum = np.sum(shap_value)

  return shap_values_sum

"""
Show the heatmap of feature and sequence
"""
def shap_feature_sequence(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))

  shap_values_sum = shap_feature_calculator()

  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=1)
  shap_values = np.sum(shap_values, axis=0)

  plt.figure(figsize=(20,2)) 
  sns.heatmap(shap_values.T, linewidth=0.0,cmap="Blues",yticklabels =config['features'])
  plt.xlabel("Sequence")
  plt.savefig(config['image_path']+'shap_feature_sequence')
  for num in range(len(values)):
    value = values[num]/shap_values_sum
    #print('Feature {0:10}:   {1:10.5f} %'.format(features[num], value*100))

"""
Show the heatmap of feature and data
"""
def shap_feature_data(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))
  
  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=2)
  shap_values = np.sum(shap_values, axis=0)

  plt.figure(figsize=(20,2)) 
  sns.heatmap(shap_values.T, linewidth=0.0,cmap="Blues",yticklabels =config['features'])
  plt.xlabel("Data")
  plt.savefig(config['image_path']+'shap_feature_data')

"""
Show the heatmap of feature and detail
"""
def shap_feature_detail(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))
  
  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=1)
  shap_values = np.sum(shap_values, axis=1)

  shap_values_add = np.add(shap_values[0:config['detail_num']], shap_values[config['detail_num']:config['detail_num']*2])

  plt.figure(figsize=(20,2)) 
  sns.heatmap(shap_values_add.T, linewidth=0.0,cmap="Blues", xticklabels=detail, yticklabels =config['features'])
  plt.xlabel("detail")
  plt.savefig(config['image_path']+'shap_feature_detail')


  detail_value_sum = np.sum(shap_values_add, axis=1)

  for i in range(config['detail_num']):
    print("detail"+str(i+1))
    for num in range(len(config['features'])):
      value = shap_values_add[i][num]/detail_value_sum[i]
      print('Feature {0:10}:   {1:10.5f} %'.format(features[num], value*100))
      print('-'*20)

"""
Show the heatmap of data and sequence
"""
def shap_data_sequence(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))

  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=3)
  shap_values = np.sum(shap_values, axis=0)

  plt.figure(figsize=(20,5)) 
  sns.heatmap(shap_values, linewidth=0.0,cmap="Blues", yticklabels=False)
  plt.xlabel("Sequence")
  plt.ylabel("Data")
  plt.savefig(config['image_path']+'shap_data_sequence')


"""
Show the heatmap of detail and sequence
"""
def shap_detail_sequence(load=True):
  if load:
    folder_path = folder
    file_name = fname
    shap_values_all = np.load(folder_path+str(file_name))

  shap_value = np.array(abs(shap_values_all))
  shap_values = np.sum(shap_value, axis=1)
  shap_values = np.sum(shap_values, axis=2)

  shap_values_add = np.add(shap_values[0:config['detail_num']], shap_values[config['detail_num']:config['detail_num']*2])

  plt.figure(figsize=(20,5)) 
  sns.heatmap(shap_values[0:config['detail_num']], linewidth=0.0,cmap="Blues", yticklabels=detail)
  plt.xlabel("Sequence")
  plt.ylabel("detail max")
  plt.savefig(config['image_path']+'detail max')


  plt.figure(figsize=(20,5)) 
  sns.heatmap(shap_values[config['detail_num']:config['detail_num']*2], linewidth=0.0,cmap="Blues", yticklabels=detail)
  plt.xlabel("Sequence")
  plt.ylabel("detail min")
  plt.savefig(config['image_path']+'detail min')

  plt.figure(figsize=(20,5)) 
  sns.heatmap(shap_values_add, linewidth=0.0,cmap="Blues", yticklabels=detail)
  plt.xlabel("Sequence")
  plt.ylabel("detail")
  plt.savefig(config['image_path']+'detail')


"""
using 'shap.npy' from 'x_test.npy' for example
"""
values = []
shap_values_sum = 0
features = config['features']
detail = []
for i in range(1,config['detail_num']+1):
  detail.append(str(i))

sequence_num = 20

# put shap npy file path, ex: 'shap.npy'
folder = config['save_path']
fname = 'shap.npy'
  
shap_feature_sequence(load=True)
shap_feature_data(load=True)
shap_feature_detail(load=True)
shap_data_sequence(load=True)
shap_detail_sequence(load=True)

shap_v=np.load(config['save_path']+'shap.npy')
print(f"shap_value_shape : { shap_v.shape}")
print('-'*20)

for iidx,i in enumerate(shap_v):
  save_path=config['save_path']+'detail'+str(iidx+1)
  if not os.path.isdir(save_path):
    os.makedirs(save_path)
  for sidx,s in enumerate(i):
    shap_df=pd.DataFrame(s)
    shap_df.set_axis(config['features'], axis='columns', inplace=True)
    dir_path=save_path+'/data'+str(sidx)+'.csv'
    shap_df.to_csv(dir_path)
  print(f'save {iidx}')